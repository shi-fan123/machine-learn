


%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from ase.db import connect
from ase.visualize import view





db = connect('organometal.db')
row = next(db.select(project='organometal'))
vars(row)








row.name


row.symbols


from IPython.display import Image
Image(filename='ABX3_perovskite.jpeg',width=400) # ACS Energy Lett. 5, 604 (2020), doi: 10.1021/acsenergylett.0c00039





db.count()  # number of rows in the database


db.count(project='organometal')  # number of rows belong to the organometal project


db.count(subproject='references')  # number of rows for references subproject





names=[]
syms=[]

for row in db.select(project='organometal'):
    names.append(row.name)
    syms.append(row.symmetry)
label_A=[name[0:2] for name in names]
label_B=[name[2:4] for name in names]
label_X3=[name[4:] for name in names]


import pandas as pd

def echo_unique(slist):
    ulist=pd.Series(slist).drop_duplicates().tolist()
    print(ulist)
    return ulist


echo_unique(label_A)
echo_unique(label_B)
echo_unique(label_X3)


unique_sym=echo_unique(syms)





for row in db.select(subproject='references'):
    if row.formula =='CH5N2':
        view(row.toatoms())





organometal_rows = [x for x in db.select(project='organometal')]
discs = [x.get('gllbsc_disc') for x in organometal_rows]
print(f"Mean of `gllbsc_disc` = {np.mean(discs):.4f}")
print(f"Variance of `gllbsc_disc` = {np.var(discs):.4f}")
plt.hist(discs)
plt.xlabel('Energy (eV)')
plt.show()


x=range(db.count(project='organometal'))
plt.scatter(x,discs)





numeric_quantities = ["energy", "gllbsc_disc", "gllbsc_dir_gap", "gllbsc_ind_gap"]
om_rows = [x for x in db.select(project='organometal')]
results=[]
fig, axs = plt.subplots(2, 2)

for prop in numeric_quantities:
    values=[row.get(prop) for row in om_rows]
    results.append(values)
    print(f"Mean of `{prop}' = {np.mean(values):.4f}")
    print(f"Variance of `{prop}' = {np.var(values):.4f}")

axs[0,0].hist(results[0])
axs[0,0].set_title(numeric_quantities[0])
axs[0,1].hist(results[1])
axs[0,1].set_title(numeric_quantities[1])
axs[1,0].hist(results[2])
axs[1,0].set_title(numeric_quantities[2])
axs[1,1].hist(results[3])
axs[1,1].set_title(numeric_quantities[3])
fig.tight_layout(pad=1.5)


numeric_quantities = ["energy", "gllbsc_disc", "gllbsc_dir_gap", "gllbsc_ind_gap"]
om_rows = [x for x in db.select(project='organometal',symmetry=unique_sym[3])]  # cubic samples
results=[]
fig, axs = plt.subplots(2, 2)

for prop in numeric_quantities:
    values=[row.get(prop) for row in om_rows]
    results.append(values)
    print(f"Mean of `{prop}' = {np.mean(values):.4f}")
    print(f"Variance of `{prop}' = {np.var(values):.4f}")

axs[0,0].hist(results[0])
axs[0,0].set_title(numeric_quantities[0])
axs[0,1].hist(results[1])
axs[0,1].set_title(numeric_quantities[1])
axs[1,0].hist(results[2])
axs[1,0].set_title(numeric_quantities[2])
axs[1,1].hist(results[3])
axs[1,1].set_title(numeric_quantities[3])
fig.tight_layout(pad=1.5)





row = db.get(name='MAPbI3', symmetry='cubic')
en_cubic = row.energy
en_refs = {}
for row in db.select(subproject='references'):
    en_refs[row.element] = row.energy / row.natoms

E_standard = en_cubic - (8 * en_refs['MA'] + en_refs['Pb'] + 3 * en_refs['I'])
print(f'hof = {E_standard / row.natoms:.3f} eV/atom')








def calculate_input_vector(row):
    symm_vec = [0, 0, 0, 0]
    A_vec = [0, 0, 0]
    B_vec = [0, 0]
    X_vec = [0, 0, 0]  # i.e I3->[0, 3, 0], I2Cl->[1, 2, 0], Br3->[0, 0, 3]
    constant = [1,]
    symm_vec[['cubic',
              'tetragonal',
              'orthorhombic_1',
              'orthorhombic_2'].index(row.symmetry)] = 1
    A_vec[['Cs', 'FA', 'MA'].index(row.name[:2])] = 1
    B_vec[0] = 1 if 'Pb' in row.name else 0
    B_vec[1] = 1 if 'Sn' in row.name else 0

    Xs = ['Cl', 'I', 'Br']
    nhalo = sum([s in Xs for s in row.symbols])
    for i, X in enumerate(Xs):
        X_vec[i] = 3 * sum([s == X for s in row.symbols]) // nhalo

    vec = symm_vec + A_vec + B_vec + X_vec + constant
    return vec





for row in db.select(symmetry='cubic', limit=5):
    print(f'name={row.name} formula={row.formula} symmetry={row.symmetry}')
    print(f'vec={calculate_input_vector(row)}')
    print('-'*79)





X = []
y = []
for row in db.select('project'):
    X.append(calculate_input_vector(row))
    y.append(row.gllbsc_ind_gap)

X = np.array(X)
y = np.array(y).reshape(-1, 1)
print('X.shape = ', np.shape(X))
print('Y.shape =', np.shape(y))





import numpy as np

def fit(X, y):
    """
    根据输入特征 X 和目标值 y 进行线性回归模型的拟合，返回权重向量 w 和损失值 loss。

    参数:
    X -- 输入特征矩阵，形状为 (n_samples, n_features)
    y -- 目标值向量，形状为 (n_samples, 1) 或 (n_samples,)

    返回:
    w -- 拟合得到的权重向量，形状为 (n_features, 1)
    loss -- 拟合过程中的平均损失值（均方误差，Mean Squared Error）
    """
    # 检查 y 的形状，如果是 (n_samples,)，则将其转换为 (n_samples, 1)
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    # 使用最小二乘法求解线性回归的权重向量 w
    # w = (X^T * X)^(-1) * X^T * y
    w = np.linalg.inv(X.T @ X) @ X.T @ y

    # 计算模型的预测值
    y_pred = X @ w

    # 计算均方误差（Mean Squared Error）
    loss = np.mean((y_pred - y) ** 2)

    return w, loss


import numpy as np

# Define the toy input
np.random.seed(0)  # Set random seed for reproducibility
n_samples = 50     # Number of samples
n_features = 5     # Number of features (same as the length of the target weight vector)

# Generate a random input feature matrix X
X_toy = np.random.rand(n_samples, n_features)

# Define the true weight vector
w_true = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Shape (n_features, 1)

# Generate the target values y using the true weight vector (y = X @ w_true)
y_toy = X_toy @ w_true

# Define the fit function
def fit(X, y):
    """
    Linear regression fit function using least squares method.

    Parameters:
    X -- Input feature matrix, shape (n_samples, n_features)
    y -- Target values, shape (n_samples, 1)

    Returns:
    w -- Weight vector, shape (n_features, 1)
    loss -- Mean squared error (MSE) loss value
    """
    # Check if y is a column vector
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    # Calculate the weight vector using least squares formula: w = (X^T * X)^(-1) * X^T * y
    w = np.linalg.inv(X.T @ X) @ X.T @ y

    # Calculate predictions
    y_pred = X @ w

    # Calculate mean squared error (MSE)
    loss = np.mean((y_pred - y) ** 2)

    return w, loss

# Fit the model on the toy input
w_est, loss = fit(X_toy, y_toy)

# Display the results
print("True weight vector:", w_true.ravel())
print("Estimated weight vector:", w_est.ravel())
print(f"Mean squared error (MSE) on the toy input: {loss:.6f}")





nsamples = 50
nfeatures = 5
X_toy = np.random.rand(250).reshape(nsamples, nfeatures)
coefficients = np.arange(1, nfeatures + 1).reshape(-1, 1)
noise = np.random.normal(scale=0.2, size=nsamples).reshape(-1, 1)
y_toy = np.dot(X_toy, coefficients) + noise
w, loss = fit(X_toy, y_toy)
plt.scatter(np.dot(X_toy, w), y_toy)
plt.plot(plt.xlim(), plt.ylim(), ls='--', color='k')

plt.show()
print(w)

















from sklearn import linear_model
linear = linear_model.LinearRegression()

linear.fit(X, y)
ybar = linear.predict(X)
ymax = np.array((y, ybar)).max() + 0.1
plt.scatter(ybar, y)
lims = [
    np.min([plt.xlim(), plt.ylim()]),
    np.max([plt.xlim(), plt.ylim()])
]
plt.plot(lims, lims, ls='--', color='k')
plt.xlabel('Predicted Band Gap [eV]')
plt.ylabel('Actual Band Gap [eV]')

# We can wrap the above in a function, to avoid typing that same code again later
def limits():
    lims = [
        np.min([plt.xlim(), plt.ylim()]),
        np.max([plt.xlim(), plt.ylim()])
    ]
    plt.plot(lims, lims, ls='--', color='k')

def make_comparison_plot(X, y, model, label):
    model.fit(X, y)
    ybar = model.predict(X)
    plt.scatter(ybar, y)
    limits()
    plt.xlabel(f'Predicted {label}')
    plt.ylabel(f'Actual {label}')








print(linear.coef_)
print(linear.intercept_)





from sklearn import linear_model

linear_regularized = linear_model.Ridge(alpha = .5)
make_comparison_plot(X, y, linear_regularized, "Band Gap [eV]")


print(linear_regularized.coef_)
print(linear_regularized.intercept_)





linear_regularized.score(X, y)





from sklearn import model_selection
folds = model_selection.KFold(n_splits=2, shuffle=True, random_state=1)
print(model_selection.cross_val_score(linear_regularized, X, y, cv=folds, scoring='explained_variance'))
print(model_selection.cross_val_score(linear, X, y, cv=folds, scoring='explained_variance'))





en_refs = {}
for row in db.select(subproject='references'):
    en_refs[row.element] = row.energy/len(row.symbols)
HoF = []
for row in db.select(project='organometal'):
        energy = row.energy
        # how many units are in there!?
        n_units = len([True for symbol in row.symbols if symbol == 'Pb' or symbol == 'Sn'])
        energy_standard = 0
        for symbol in row.symbols:
            if symbol in ['Cs','Pb','Sn','I','Br','Cl']:
                energy_standard += en_refs[symbol]
        if 'FA' in row.name:
            energy_standard += n_units * en_refs['FA'] * 8
        if 'MA' in row.name:
            energy_standard += n_units * en_refs['MA'] * 8
        HoF.append((energy-energy_standard) / n_units)








# Define `X_hof` and `y_hof` in here






from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(?, ?)






import graphviz
feature_names = ['first_cs','first_FA','first_MA','lead','tin','chlorine','iodine','bromine','reg']
target_names = ['cubic', 'tetragonal', 'orthorhombic_1', 'orthorhombic_2']
dot_data = tree.export_graphviz(clf, out_file=None,
                                feature_names = feature_names,
                                class_names = target_names,
                                filled=True, rounded=True,
                                special_characters=True)
graph = graphviz.Source(dot_data)
graph





folds = model_selection.KFold(n_splits=4, shuffle=True, random_state=0)
scores = []
for max_depth in range(1, 10):
    clf = tree.DecisionTreeClassifier(max_depth=max_depth)
    score = model_selection.cross_val_score(clf, X_hof, y_hof, cv=folds)
    print(max_depth, score)
    scores.append(score.mean())
plt.plot(range(1, 10), scores)
plt.xlabel('Maximum depth')
plt.ylabel('Cross-validation score')
plt.show()

















from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

kernel = RBF(length_scale=0.1)
model = GaussianProcessRegressor(kernel=kernel)
model.fit(X, y)
fit = model.predict(X)
model.score(X, y)

















# Determine all the materials in the database, that do not have the cubic symmetry calculated
names = set(r.name for r in db.select("symmetry!=cubic", project="organometal")) - \
        set(r.name for r in db.select(project="organometal", symmetry="cubic"))







row = next(db.select(name='similar name', symmetry='similar symmetry'))
atoms = row.toatoms()
symbols = atoms.get_chemical_symbols()
new_symbols = [? if symbol == ? else symbol
               for symbol in symbols]
atoms.set_chemical_symbols(new_symbols)
view(atoms)
atoms.write("chosen_material.xyz")





#%%writefile myrelax.py
from gpaw import GPAW, FermiDirac, PW
from ase.io import read
from ase.optimize.bfgs import BFGS
from ase.filters import UnitCellFilter

atoms = read('chosen_material.xyz')
name = atoms.get_chemical_formula()
calc = GPAW(mode=PW(500),
            kpts={'size': (4, 4, 4), 'gamma': True},
            xc='PBE',
            txt=name + '_relax.out',
            occupations=FermiDirac(width=0.05))

atoms.calc = calc
uf = UnitCellFilter(atoms, mask=[1, 1, 1, 0, 0, 0])
relax = BFGS(uf, logfile=name + '_relax.log', trajectory='chosen_relax.traj')
relax.run(fmax=0.05)  # force is really a stress here





#%%writefile dft-gllb.py
from ase.io import read
from gpaw import GPAW, FermiDirac, PW

atoms = read('chosen_relax.traj')
calc = GPAW(mode=PW(500),
            kpts={'size': (8, 8, 8), 'gamma': True},
            xc='GLLBSC',
            occupations=FermiDirac(width=0.05))

atoms.calc = calc
energy = atoms.get_potential_energy()

# Note! An accurate discontinuity calculation requires a k-point grid that
# gives accurate HOMO/VBM and LUMO/CBM levels (in other words, the k-points of
# the valence band maximum and the conduction band minimum should be
# included in the used k-point grid).
homo, lumo = calc.get_homo_lumo()
response = calc.hamiltonian.xc.response
dxc_pot = response.calculate_discontinuity_potential(homo, lumo)
KS_gap, dxc = response.calculate_discontinuity(dxc_pot)
gap = KS_gap + dxc
print(f"The gap is {gap:.3f} with components: Kohn-Sham gap {KS_gap:.3f} and "
      f"discontinuity gap of {dxc:.3f}.")



